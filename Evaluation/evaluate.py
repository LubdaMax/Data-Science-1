import numpy as np
import pandas as pd
import os
import nltk
from nltk.corpus import stopwords
from nltk import f_measure
from nltk.metrics.scores import recall
from nltk.metrics.scores import precision
from pathlib import Path
import re
import pickle
import string
from rouge import Rouge
import sklearn



rootpath = Path.cwd()

## open data for reference summaries:
open_cnn_summ = open(Path.joinpath(rootpath, r"Pre-Processing & EDA\cnn_summaries_dict"), 'rb')
cnn_summary = pickle.load(open_cnn_summ)
open_cnn_summ.close()

open_wiki_summ = open(Path.joinpath(rootpath, r"Pre-Processing & EDA\wiki_partial_summaries_10k"), 'rb')
wiki_summary = pickle.load(open_wiki_summ)
open_wiki_summ.close()


# open data for output summaries generated by TEXTRANK
openfile = open(Path.joinpath(rootpath, r"TextRank\wiki_TRoutput_summ_dict_lemm_3sent_inclDigits"), 'rb')
wiki_data = pickle.load(openfile)
openfile.close()

openfile = open(Path.joinpath(rootpath, r"TextRank\cnn_TRoutput_summ_dict_lemm_3sent_inclDigits"), 'rb')
cnn_data = pickle.load(openfile)
openfile.close()



desired_width = 500
pd.set_option('display.width', desired_width)
np.set_printoptions(linewidth=desired_width)
pd.set_option('display.max_columns', 10)



def get_summ(dictionary):
    Summaries = []
    for i in range(len(dictionary)):
        Summary = ""

        for s in range(dictionary["Summary{0}".format(i)].shape[0]):
            Summary += dictionary["Summary{0}".format(i)].iloc[s, 0]
            Summary += " "

        Summary = Summary[:-1]
        Summaries.append(Summary)
    return Summaries


def get_summ_wiki(dictionary):
    Summaries = []

    for i in range(len(dictionary)):
        Summary = dictionary["Summary{0}".format(i)]
        Summaries.append(Summary)
    return Summaries


def create_summ_TR(ranked_sentences_dict, article_key, article_data, number_sentences):
    """ generates output summary specifically for the TextRank Output

    :param ranked_sentences_dict: dictionary, that ranks importance sentences
    :param article_key:
    :param article_data:
    :param number_sentences: number of sentences in Summary
    :return:
    """
    summary = []
    for i in range(number_sentences):
        s = ranked_sentences_dict[article_key][i][0]
        summary.append(article_data[article_key].iloc[s, 0])
        summary.append(" ")

        summary = "".join(summary)

    return summary


def eval_summ(cnn_summary, cnn_data, wiki_summary, wiki_data):
    rouge = Rouge()

    cnn_score_frame = pd.DataFrame(columns=["r1-f", "r1-p", "r1-r", "r2-f", "r2-p", "r2-r", "rl-f", "rl-p", "rl-r"])
    for i in range(len(cnn_summary)):
        # score = recall(nltk.sent_tokenize(cnn_summ[i]), nltk.sent_tokenize(cnn_generated_summ[i]))
        if (len(cnn_summary[i]) and len(cnn_data[i])) > 0:
            try:
                score = rouge.get_scores(cnn_summary[i], cnn_data[i])

                cnn_score_frame = cnn_score_frame.append(pd.DataFrame(
                    {'r1-f': score[0]['rouge-1']['f'], 'r1-p': score[0]['rouge-1']['p'],
                     'r1-r': score[0]['rouge-1']['r'], 'r2-f': score[0]['rouge-2']['f'],
                     'r2-p': score[0]['rouge-2']['p'], 'r2-r': score[0]['rouge-2']['r'],
                     'rl-f': score[0]['rouge-l']['f'], 'rl-p': score[0]['rouge-l']['p'],
                     'rl-r': score[0]['rouge-l']['r']}, index=[0]), ignore_index=True)
            except:
                print("summary: ", cnn_summary[i])
                print("generated: ", cnn_data[i])
                print(
                    "------------------------------------------------------------------------------------------------------------------------")

    wiki_score_frame = pd.DataFrame(columns=["r1-f", "r1-p", "r1-r", "r2-f", "r2-p", "r2-r", "rl-f", "rl-p", "rl-r"])
    for i in range(len(wiki_summary)):
        print(i)
        # score = recall(nltk.sent_tokenize(cnn_summ[i]), nltk.sent_tokenize(cnn_generated_summ[i]))
        print(len(wiki_summary[i]))
        print(len(wiki_data[i]))
        if (len(wiki_summary[i]) and len(wiki_data[i])) > 0:
            score = rouge.get_scores(wiki_summary[i], wiki_data[i])

            wiki_score_frame = wiki_score_frame.append(pd.DataFrame(
                {'r1-f': score[0]['rouge-1']['f'], 'r1-p': score[0]['rouge-1']['p'], 'r1-r': score[0]['rouge-1']['r'],
                 'r2-f': score[0]['rouge-2']['f'], 'r2-p': score[0]['rouge-2']['p'], 'r2-r': score[0]['rouge-2']['r'],
                 'rl-f': score[0]['rouge-l']['f'], 'rl-p': score[0]['rouge-l']['p'], 'rl-r': score[0]['rouge-l']['r']},
                index=[0]), ignore_index=True)

    return wiki_score_frame, cnn_score_frame



## EVALUATION
## generate input required for evaulation method: TEXTRANK
cnn_summ = get_summ(cnn_summary)
wiki_summ = get_summ_wiki(wiki_summary)
cnn_generated_summ = get_summ(cnn_data)
wiki_generated_summ = get_summ(wiki_data)


##evaluate performance TEXTRANK
wiki_score, cnn_score = eval_summ(cnn_summ, cnn_generated_summ, wiki_summ, wiki_generated_summ)
print(wiki_score.head(), wiki_score.shape)
print("r1-f mean: ", wiki_score["r1-f"].mean())
print("r1-p mean: ", wiki_score["r1-p"].mean())
print("r1-r mean: ", wiki_score["r1-r"].mean())
print("r2-f mean: ", wiki_score["r2-f"].mean())
print("r2-p mean: ", wiki_score["r2-p"].mean())
print("r2-r mean: ", wiki_score["r2-r"].mean())
print("rl-f mean: ", wiki_score["rl-f"].mean())
print("rl-p mean: ", wiki_score["rl-p"].mean())
print("rl-r mean: ", wiki_score["rl-r"].mean())
print(cnn_score.head(), cnn_score.shape)
print("r1-f mean: ", cnn_score["r1-f"].mean())
print("r1-p mean: ", cnn_score["r1-p"].mean())
print("r1-r mean: ", cnn_score["r1-r"].mean())
print("r2-f mean: ", cnn_score["r2-f"].mean())
print("r2-p mean: ", cnn_score["r2-p"].mean())
print("r2-r mean: ", cnn_score["r2-r"].mean())
print("rl-f mean: ", cnn_score["rl-f"].mean())
print("rl-p mean: ", cnn_score["rl-p"].mean())
print("rl-r mean: ", cnn_score["rl-r"].mean())

